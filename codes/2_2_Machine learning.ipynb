{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "519f70a9",
   "metadata": {},
   "source": [
    "# Machine learning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e08eb28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.model_selection import LeaveOneGroupOut, StratifiedGroupKFold, GroupKFold, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, cohen_kappa_score\n",
    "from sklearn.metrics import matthews_corrcoef, make_scorer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87b0d4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    \"\"\"\n",
    "        Preparar los datos en los formatos requeridos (arrays, lista o dataframe)\n",
    "        Returns: X, y, groups, filenames\n",
    "    \"\"\"\n",
    "    y = np.array(df['label'].values)\n",
    "    groups = list(df['user'].values)\n",
    "    filenames = np.array(list(df['user'].values))\n",
    "    X = df.loc[:,'WC':].copy()\n",
    "    return X, y, groups, filenames\n",
    "\n",
    "def pred_speaker(filenames, y_true, y_pred, threshold):\n",
    "    \"\"\"\n",
    "        Calcular la prediccion por speaker a partir de la prediccion de cada uno de sus segmentos.\n",
    "        Para ello, calcula la media de sus predicciones y, si es > 0.5 le asigna 1, si es < es 0.\n",
    "    \"\"\"\n",
    "    y_trues_speaker = []\n",
    "    y_preds_speaker = []\n",
    "    predmeans_speaker = []\n",
    "    for file in set(filenames):\n",
    "        indexes = np.where(np.array(filenames) == file)[0]\n",
    "        preds = list(np.array(y_pred)[indexes])\n",
    "        predmean = np.mean(preds)\n",
    "        predmeans_speaker.append(predmean)\n",
    "        if(predmean > threshold):\n",
    "            y_preds_speaker.append(1)\n",
    "        else:\n",
    "            y_preds_speaker.append(0)\n",
    "        y_trues_speaker.append(y_true[indexes[0]])\n",
    "    return y_trues_speaker, y_preds_speaker, predmeans_speaker\n",
    "\n",
    "def unison_shuffled_copies(X, y, filenames):\n",
    "    \"\"\"\n",
    "        Shuffle los datos de forma pareada, manteniendo la correspondencia de filas en X, y filenames.\n",
    "    \"\"\"\n",
    "    assert len(X) == len(y)\n",
    "    assert len(y) == len(filenames)\n",
    "    np.random.seed(13)\n",
    "    p = np.random.permutation(len(X))\n",
    "    return X[p], y[p], filenames[p]\n",
    "\n",
    "def makeDirIfNotExists(dir):\n",
    "    if not os.path.isdir(dir):\n",
    "        os.mkdir(dir)\n",
    "        \n",
    "def load_combinations():\n",
    "    \"\"\"\n",
    "        Cargar los diccionarios con los clasificadores, la combinacion de parámetros a probar y las métricas de evaluación.\n",
    "    \"\"\"\n",
    "    dict_classifiers = {\n",
    "            \"SVC\": SVC(C=1, kernel= 'linear', gamma = 'scale'),\n",
    "            \"KNN\": KNN(n_neighbors=2, weights='uniform', metric='minkowski'),\n",
    "    }\n",
    "    dict_parameters = {\n",
    "            \"SVC\": {'SVC__kernel': ['rbf','sigmoid','poly','linear'],\n",
    "                    'SVC__gamma': [0.001, 0.01, 0.1, 1, 'auto','scale'],\n",
    "                    'SVC__C': [1, 10, 100, 1000]},\n",
    "            \"KNN\": {'KNN__n_neighbors': [1,2,3,5,7],\n",
    "                    'KNN__weights': ['uniform','distance'],\n",
    "                    'KNN__metric': ['euclidean','manhattan','minkowski']}\n",
    "            }\n",
    "    \n",
    "    scoring = {'kappa': make_scorer(cohen_kappa_score), 'acc': 'accuracy', 'mcc': make_scorer(matthews_corrcoef)}\n",
    "    \n",
    "    return dict_classifiers, dict_parameters, scoring\n",
    "\n",
    "def load_train():\n",
    "    info_train_cc = pd.read_csv('C:/Users/lugoza/Documents/AnacondaFiles/Dementia/data/ADReSS-IS2020/train/cc_meta_data.txt',sep=';')\n",
    "    info_train_cd = pd.read_csv('C:/Users/lugoza/Documents/AnacondaFiles/Dementia/data/ADReSS-IS2020/train/cd_meta_data.txt',sep=';')\n",
    "\n",
    "    info_train_cc['Label'] = 0 \n",
    "    info_train_cd['Label'] = 1\n",
    "\n",
    "    info_train = pd.concat([info_train_cc, info_train_cd])\n",
    "    info_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    info_train = info_train.rename(columns={'ID   ': 'ID', ' age': 'age', ' gender ':'gender'})\n",
    "    info_train.loc[0,'mmse'] = np.nan\n",
    "    info_train[\"mmse\"] = pd.to_numeric(info_train[\"mmse\"])\n",
    "    \n",
    "    return info_train\n",
    "\n",
    "def list_drop_subjects(info_train):\n",
    "    subjects_drop_mal = list(info_train.loc[(info_train.mmse>23)&(info_train.Label == 1)]['ID'].values)\n",
    "    subjects_drop = []\n",
    "    for s in subjects_drop_mal:\n",
    "        subjects_drop.append(s[:-1])\n",
    "    return subjects_drop\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "        A partir de las predicciones y las ground truth, calcula diferentes métricas.\n",
    "        (si se le pasan las labels por sujeto, las métricas son por sujeto)\n",
    "        https://towardsdatascience.com/should-i-look-at-precision-recall-or-specificity-sensitivity-3946158aace1\n",
    "    \"\"\"\n",
    "    \n",
    "    acc = round(accuracy_score(y_true, y_pred)*100,2)\n",
    "    kappa = round(cohen_kappa_score(y_true,y_pred),3)\n",
    "    cm = confusion_matrix(y_true, y_pred) \n",
    "    tn = cm[0][0] # True negative => negative = non-AD = 0 (control)\n",
    "    fp = cm[0][1]\n",
    "    fn = cm[1][0]\n",
    "    tp = cm[1][1] # True positive => positive = AD = 1 (dementia)\n",
    "    sensitivity = round(tp/(tp+fn),3) # = recall\n",
    "    specificity = round(tn/(tn+fp),3)\n",
    "    precision = round(tp/(tp+fp),3)\n",
    "    recall = round(tp/(tp+fn),3)\n",
    "    f1score = round(2*(precision * recall)/(precision + recall),3) # = f1_score(y_true,y_pred)\n",
    "    \n",
    "    return acc, kappa, f1score, cm, sensitivity, specificity, precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758f970e",
   "metadata": {},
   "source": [
    "# Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3239b977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ASR_wav2vec2_noPunc_LIWC2015.xlsx', 'ASR_whisper_noPunc_LIWC2015.xlsx', 'ASR_whisper_noPunc_LIWC2015_pause.xlsx', 'ASR_whisper_Punc_LIWC2015.xlsx', 'ASR_whisper_Punc_LIWC2015_pause.xlsx', 'manual_noPunc_LIWC2015.xlsx', 'manual_noPunc_LIWC2015_pause.xlsx', 'manual_Punc_LIWC2015.xlsx', 'manual_Punc_LIWC2015_pause.xlsx']\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir('0-publication/features/')\n",
    "all_features = [f for f in files if f.endswith('xlsx')]\n",
    "print(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81035278",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save = True \n",
    "path_save = '0-publication/results_AASPAA/'\n",
    "tic_total = time.time()\n",
    "df_results = pd.DataFrame()\n",
    "# ['ASR_wav2vec2_noPunc_LIWC2015','ASR_whisper_noPunc_LIWC2015','ASR_whisper_noPunc_LIWC2015_pause','ASR_whisper_Punc_LIWC2015','ASR_whisper_Punc_LIWC2015_pause']\n",
    "for cleaning_training in [False]:\n",
    "    if save:\n",
    "        makeDirIfNotExists(path_save)\n",
    "    for name_features in all_features: \n",
    "        # (1) Load features \n",
    "        print('Loading features ...')\n",
    "        df_features_all = pd.read_excel('0-publication/features/'+name_features)\n",
    "                \n",
    "        if cleaning_training:\n",
    "            info_train = load_train()\n",
    "            subjects_drop = list_drop_subjects(info_train)\n",
    "            df_features = df_features_all[~df_features_all['user'].isin(subjects_drop)].copy()\n",
    "            df_features.reset_index(inplace=True, drop=True)\n",
    "            print('df_features:',df_features.shape)\n",
    "        else:\n",
    "            df_features = df_features_all.copy()\n",
    "\n",
    "        df_dev = df_features.loc[df_features.partition == 'train'].copy()\n",
    "        df_dev.reset_index(inplace=True, drop=True)\n",
    "        X_dev, y_dev, groups_dev, filenames_dev = prepare_data(df_dev)\n",
    "        df_test = df_features.loc[df_features.partition == 'test'].copy()\n",
    "        df_test.reset_index(inplace=True, drop=True)\n",
    "        X_test, y_test, groups_test, filenames_test = prepare_data(df_test)\n",
    "\n",
    "        # (2) Feature selection \n",
    "        print('Feature selection...')\n",
    "        tic = time.time()\n",
    "        # (2.1) Feature normalization => solo para este paso!\n",
    "        std_scaler = StandardScaler() \n",
    "        X_dev_norm = std_scaler.fit_transform(X_dev)\n",
    "        # (2.2) Recursive Feature Elimination (RFE) \n",
    "        selector = RFE(SVC(kernel=\"linear\"), n_features_to_select=1, step=1, verbose=0)\n",
    "        selector.fit(X_dev_norm, y_dev)\n",
    "        sortedFeatures = sorted(zip(selector.ranking_, X_dev.columns))\n",
    "        bestFeatures = [] # Quedarse solo con el nombre de las features ya ordenadas\n",
    "        for sf in sortedFeatures:\n",
    "            bestFeatures.append(sf[1])\n",
    "        toc = time.time()\n",
    "        print('    Duration:',round((toc-tic)/60,2),'min')\n",
    "\n",
    "        # (3) Training models \n",
    "        print('Training models...')\n",
    "\n",
    "        dict_classifiers = {\n",
    "                    \"SVC\": SVC(C=1, kernel= 'linear', gamma = 'scale'),\n",
    "                    \"KNN\": KNN(n_neighbors=5, weights='uniform', metric='minkowski'),\n",
    "            }\n",
    "        for clas in dict_classifiers.keys():\n",
    "            tic_clas = time.time()\n",
    "            print('   ', clas)\n",
    "            dictionary_nr = dict()\n",
    "            for nr in range(1,len(sortedFeatures)+1):\n",
    "                tic_nr = time.time()\n",
    "                topNrFeatures = bestFeatures[0:nr]\n",
    "                seleccion_X_dev = np.array(X_dev.loc[:, topNrFeatures]) # X_dev sin normalizar, se hace despues\n",
    "                seleccion_X_test = np.array(X_test.loc[:, topNrFeatures])\n",
    "\n",
    "                # (3.1) Cross-validation\n",
    "                logo = LeaveOneGroupOut()\n",
    "                i = 0; y_pred_tots = []; y_true_tots = []\n",
    "                for train_index, val_index in logo.split(seleccion_X_dev, y_dev, groups_dev):\n",
    "                    X_train_i, X_val_i = seleccion_X_dev[train_index], seleccion_X_dev[val_index]\n",
    "                    y_train_i, y_val_i = y_dev[train_index], y_dev[val_index]\n",
    "                    filenames_train_i, filenames_val_i = filenames_dev[train_index], filenames_dev[val_index]\n",
    "\n",
    "                    # Shuffle data\n",
    "                    X_train, y_train, filenames_train = unison_shuffled_copies(X_train_i, y_train_i, filenames_train_i)\n",
    "                    X_val, y_val, filenames_val = unison_shuffled_copies(X_val_i, y_val_i, filenames_val_i)\n",
    "\n",
    "                    # Normalize data\n",
    "                    new_scaler = StandardScaler() #StandardScaler(), MinMaxScaler\n",
    "                    X_train_norm = new_scaler.fit_transform(X_train)\n",
    "                    X_val_norm = new_scaler.transform(X_val)\n",
    "\n",
    "                    clf = dict_classifiers[clas]\n",
    "                    clf.fit(X_train_norm, y_train)\n",
    "                    cv_y_pred, cv_y_true = clf.predict(X_val_norm), y_val\n",
    "\n",
    "                    subject_y_true = int(cv_y_true[0])\n",
    "                    subject_y_pred = int(cv_y_pred[0])\n",
    "\n",
    "                    y_true_tots.append(subject_y_true)\n",
    "                    y_pred_tots.append(subject_y_pred)  \n",
    "                    \n",
    "                    i+=1 \n",
    "                \n",
    "                predictions_nr = dict()\n",
    "                predictions_nr['subject_y_true'] = y_true_tots\n",
    "                predictions_nr['subject_y_pred'] = y_pred_tots\n",
    "                dictionary_nr[str(nr)] = predictions_nr\n",
    "            \n",
    "            for nr in range(1,len(sortedFeatures)+1):\n",
    "\n",
    "                y_true_tots = dictionary_nr[str(nr)]['subject_y_true']\n",
    "                y_pred_tots = dictionary_nr[str(nr)]['subject_y_pred']\n",
    "                \n",
    "                # (3.2) Calculate metrics CV (per speaker)  \n",
    "                cv_acc, cv_kappa, cv_f1score, cv_cm, cv_sensitivity, cv_specificity, cv_precision = compute_metrics(y_true_tots, y_pred_tots)\n",
    "\n",
    "                # (4) Testing\n",
    "                topNrFeatures = bestFeatures[0:nr]\n",
    "                seleccion_X_dev = np.array(X_dev.loc[:, topNrFeatures]) # X_dev sin normalizar, se hace despues\n",
    "                seleccion_X_test = np.array(X_test.loc[:, topNrFeatures])\n",
    "\n",
    "                std_scaler = StandardScaler() #StandardScaler(), MinMaxScaler\n",
    "                X_dev_norm = std_scaler.fit_transform(seleccion_X_dev)\n",
    "                X_test_norm = std_scaler.transform(seleccion_X_test)\n",
    "\n",
    "                clf_test = dict_classifiers[clas]\n",
    "                clf_test.fit(X_dev_norm, y_dev)\n",
    "                test_y_true, test_y_pred = y_test, clf_test.predict(X_test_norm) \n",
    "                test_acc, test_kappa, test_f1score, test_cm, test_sensitivity, test_specificity, test_precision = compute_metrics(test_y_true, test_y_pred)\n",
    "                \n",
    "                errors = []\n",
    "                for i in range(len(groups_test)):\n",
    "                    if test_y_true[i] != test_y_pred[i]:\n",
    "                        user = groups_test[i]\n",
    "                        errors.append(user)\n",
    "        \n",
    "                new_row ={'features':name_features,'n_features':nr,\n",
    "                          'model': clas,'params':clf.get_params(),\n",
    "                          'cleaning_training':cleaning_training, \n",
    "\n",
    "                          'cv_acc':cv_acc,'cv_kappa':cv_kappa, 'cv_f1score':cv_f1score,'cv_cm':cv_cm,\n",
    "                          'cv_sensitivity':cv_sensitivity,'cv_specifity':cv_specificity,'cv_precision':cv_precision,\n",
    "\n",
    "                          'test_acc':test_acc,'test_kappa':test_kappa,'test_f1score':test_f1score,'test_cm':test_cm,\n",
    "                          'test_sensitivity':test_sensitivity,'test_specifity':test_specificity,'test_precision':test_precision, \n",
    "\n",
    "                          'top_features':topNrFeatures, 'errors_test':errors\n",
    "                         }\n",
    "                df_new_row = pd.DataFrame.from_records([new_row]) \n",
    "                df_results = pd.concat([df_results, df_new_row]) \n",
    "\n",
    "            toc_clas = time.time()\n",
    "            print('    Duration:',round((toc_clas-tic_clas)/60,2),'min')\n",
    "            if save:\n",
    "                df_results.to_csv(path_save+'resultados_AASPAA_'+name_features+'.csv',index=False)\n",
    "                df_results.to_excel(path_save+'resultados_AASPAA'+name_features+'.xlsx',index=False)\n",
    "\n",
    "toc_total = time.time()\n",
    "print('Total duration:',round((toc_total - tic_total)/60,2),'min') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b950e1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_features = ['ASR_wav2vec2_noPunc_LIWC2015.xlsx', 'ASR_whisper_noPunc_LIWC2015.xlsx', \n",
    "                'ASR_whisper_noPunc_LIWC2015_pause.xlsx', 'ASR_whisper_Punc_LIWC2015.xlsx', \n",
    "                'ASR_whisper_Punc_LIWC2015_pause.xlsx', 'manual_noPunc_LIWC2015.xlsx', \n",
    "                'manual_noPunc_LIWC2015_pause.xlsx', 'manual_Punc_LIWC2015.xlsx', 'manual_Punc_LIWC2015_pause.xlsx']\n",
    "path_save = '0-publication/results_AASPAA/'\n",
    "new_df = pd.DataFrame(columns = df_results.columns)\n",
    "for features in all_features:\n",
    "    for model in ['SVC','KNN']:\n",
    "        for cleaning_training in [False]:\n",
    "            df_select = df_results.loc[(df_results.features == features)&(df_results.model == model)&\n",
    "                           (df_results.cleaning_training == cleaning_training)].copy()\n",
    "            df_select.reset_index(inplace=True, drop=True)\n",
    "            a = pd.DataFrame(df_select.sort_values(by='cv_acc',ascending=False).iloc[0]).T\n",
    "            new_df = pd.concat([new_df,a])\n",
    "# new_df.to_excel(path_save+'resultados_AASPAA_summary.xlsx',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
