{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIWO4i-wqjvl"
   },
   "source": [
    "# Word embeddings and neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QzDckGsgqiag"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, cohen_kappa_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "import spacy\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import keras\n",
    "from keras import callbacks\n",
    "from keras.models import Sequential, Input\n",
    "from keras.layers import Dense, Bidirectional\n",
    "from keras.layers import Dropout, SpatialDropout1D\n",
    "from keras.layers import LSTM, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.layers import Embedding, Flatten, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import LearningRateScheduler as LRS\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDirIfNotExists(dir):\n",
    "    if not os.path.isdir(dir):\n",
    "        os.mkdir(dir)\n",
    "        \n",
    "def load_all_embeddings(path, filename):\n",
    "    # Let's make a dict mapping words (strings) to their NumPy vector representation:\n",
    "    if filename.split('/')[0] == 'word2vec':\n",
    "        embeddings_index = KeyedVectors.load_word2vec_format(path + filename, binary=True)\n",
    "        n_word_vectors = len(embeddings_index.key_to_index.keys())\n",
    "    else:\n",
    "        embeddings_index = {}\n",
    "        with open(path + filename, encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                word, coefs = line.split(maxsplit=1)\n",
    "                coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "                embeddings_index[word] = coefs\n",
    "        n_word_vectors = len(embeddings_index) \n",
    "    # print('Number of words in the embeddings:',n_word_vectors)\n",
    "    return embeddings_index\n",
    "\n",
    "def load_embedding(filename, embeddings_index, tokenizer):\n",
    "    # https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "    '''\n",
    "        returns:\n",
    "            num_tokens: Number of different words in the data\n",
    "            misses: Number of words that are not in the embedding\n",
    "            misses_words: Words that are not in the embedding\n",
    "    '''\n",
    "    # Let's prepare a corresponding embedding matrix that we can use in a Keras Embedding layer. \n",
    "    # It's a matrix where entry at index i is the pre-trained vector for the word of index i in our vectorizer's vocabulary.\n",
    "    num_tokens = len(tokenizer.word_index) + 1 # len(voc) + 2\n",
    "    embedding_dim = int(filename.split('.')[-2].split('_')[-1][0:-1]) #embedding_dim = int(filename.split('.')[2][0:-1])\n",
    "    hits = 0; misses = 0; misses_words = list()\n",
    "    \n",
    "    # Prepare embedding matrix\n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "    for word, i in tokenizer.word_index.items(): \n",
    "        if filename.split('/')[0] == 'word2vec':\n",
    "            try:\n",
    "                embedding_vector = embeddings_index[word] #embeddings_index.get(word)\n",
    "            except:\n",
    "                embedding_vector = None\n",
    "        else:\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            \n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            # This includes the representation for \"padding\" and \"OOV\"\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "            misses_words.append(word)\n",
    "    # print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "    percentage_misses = round((misses*100)/(hits+misses),2)\n",
    "    # print('Found %s unique tokens.' % len(tokenizer.word_index))\n",
    "    \n",
    "    return num_tokens, percentage_misses, misses_words, embedding_dim, embedding_matrix\n",
    "\n",
    "def prepare_data(max_nb_words, max_sequence_length, filters, lower, X_train_df, Y_train_df, X_dev_df, Y_dev_df, X_test_df, Y_test_df):\n",
    "    \n",
    "    # max_nb_words = The maximum number of words to be used (most frequent) -> size of the vocabulary   \n",
    "    # filters = String where each element is a character that will be filtered from the texts. \n",
    "    #           The default is all punctuation, plus tabs and line breaks, minus the ' character\n",
    "    # lower = Whether to convert the texts to lowercase \n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "    \n",
    "    # max_sequence_length = Max number of words in each complaint.\n",
    "    \n",
    "    # Tokenize and pad sequences\n",
    "    tokenizer = Tokenizer(num_words=max_nb_words, filters=filters, lower=lower, split=' ', char_level=False, oov_token=None) \n",
    "    tokenizer.fit_on_texts(X_train_df.values) # Updates internal vocabulary based on a list of texts\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    # print('Found %s unique tokens.' % len(word_index))\n",
    "    # print('Maximum tokens used:', max_nb_words)\n",
    "\n",
    "    X_train = tokenizer.texts_to_sequences(X_train_df.values) # Transforms each text in texts to a sequence of integers\n",
    "    X_train = pad_sequences(X_train, maxlen=max_sequence_length) # Pads/truncates sequences to the same length (padding='pre', truncating='pre')\n",
    "    Y_train = Y_train_df\n",
    "    \n",
    "    try:\n",
    "        X_dev = tokenizer.texts_to_sequences(X_dev_df.values)\n",
    "    except:\n",
    "        print('One subject in validation')\n",
    "        X_dev = tokenizer.texts_to_sequences(X_dev_df)\n",
    "    X_dev = pad_sequences(X_dev, maxlen=max_sequence_length)\n",
    "    Y_dev = Y_dev_df\n",
    "\n",
    "    X_test = tokenizer.texts_to_sequences(X_test_df.values)\n",
    "    X_test = pad_sequences(X_test, maxlen=max_sequence_length)\n",
    "    Y_test = Y_test_df\n",
    "\n",
    "    print('\\nShape of training: X =', X_train.shape,' Y =',Y_train.shape)\n",
    "    print('Shape of validation: X =', X_dev.shape,' Y =',Y_dev.shape)\n",
    "    print('Shape of test: X =', X_test.shape,' Y =',Y_test.shape)\n",
    "\n",
    "    return X_train, Y_train, X_dev, Y_dev, X_test, Y_test, tokenizer\n",
    "    \n",
    "def load_model(embedding_dim, num_tokens, embedding_matrix):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_tokens, embedding_dim, weights=[embedding_matrix], input_length=X_train.shape[1], trainable=False))\n",
    "    model.add(Bidirectional(LSTM(128,return_sequences=True,dropout=0.2)))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(64,activation='relu')) \n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "        A partir de las predicciones y las ground truth, calcula diferentes métricas.\n",
    "        (si se le pasan las labels por sujeto, las métricas son por sujeto)\n",
    "        https://towardsdatascience.com/should-i-look-at-precision-recall-or-specificity-sensitivity-3946158aace1\n",
    "    \"\"\"\n",
    "    \n",
    "    acc = round(accuracy_score(y_true, y_pred)*100,2)\n",
    "    kappa = round(cohen_kappa_score(y_true,y_pred),3)\n",
    "    cm = confusion_matrix(y_true, y_pred) \n",
    "    tn = cm[0][0] # True negative => negative = non-AD = 0 (control)\n",
    "    fp = cm[0][1]\n",
    "    fn = cm[1][0]\n",
    "    tp = cm[1][1] # True positive => positive = AD = 1 (dementia)\n",
    "    sensitivity = round(tp/(tp+fn),3) # = recall\n",
    "    specificity = round(tn/(tn+fp),3)\n",
    "    precision = round(tp/(tp+fp),3)\n",
    "    recall = round(tp/(tp+fn),3)\n",
    "    f1score = round(2*(precision * recall)/(precision + recall),3) # = f1_score(y_true,y_pred)\n",
    "    \n",
    "    return acc, kappa, f1score, cm, sensitivity, specificity, precision\n",
    "\n",
    "def save_test_results(path_save,identifier,y_true,y_pred,df_test):\n",
    "    df_errors = pd.DataFrame(columns=['user','predicted','true','errores'])\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == y_true[i]:\n",
    "            error = 'acierto'\n",
    "        else:\n",
    "            error = 'fallo'\n",
    "\n",
    "        new_row = {'user': df_test.loc[i,'file'],\n",
    "                 'predicted':y_pred[i],\n",
    "                 'true':y_true[i],\n",
    "                 'errores':error}\n",
    "        df_errors = df_errors.append(new_row, ignore_index=True)\n",
    "    df_errors.to_csv(path_save+str(identifier)+'_test_results.csv', index = False)\n",
    "    \n",
    "def print_evaluations(path_save, identifier, history, save = False):\n",
    "    \n",
    "    #NUMERO DE EPOCHS\n",
    "    num_epochs = range(1, len(history.history['loss']) + 1)\n",
    "\n",
    "    #GRAFICO DEL ENTRENAMIENTO LOSS\n",
    "    fig = plt.figure()\n",
    "    plt.plot(num_epochs, history.history['loss'], 'r--')\n",
    "    plt.plot(num_epochs, history.history['val_loss'], 'b-')\n",
    "    plt.legend(['Loss Entrenamiento', 'Loss Validacion'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    if save:\n",
    "        plt.savefig(path_save+str(identifier)+'_loss.jpeg')\n",
    "    plt.show()\n",
    "\n",
    "    #GRAFICO DEL ENTRENAMIENTO ACCURACY\n",
    "    fig = plt.figure()\n",
    "    plt.plot(num_epochs, history.history['accuracy'], 'r--')\n",
    "    plt.plot(num_epochs, history.history['val_accuracy'], 'b-')\n",
    "    plt.legend(['Accuracy Entrenamiento', 'Accuracy Validacion'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    if save:\n",
    "        plt.savefig(path_save+str(identifier)+'_acc.jpeg')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path = 'pretrained_embeddings/'\n",
    "my_embeddings = ['fasttext/crawl-2M_300d.vec'] \n",
    "print('[INFO] Loading embeddings...')\n",
    "start_emb = time.time()\n",
    "all_embeddings = dict()\n",
    "for filename in my_embeddings:\n",
    "    embeddings_index = load_all_embeddings(path, filename)\n",
    "    all_embeddings[filename] = embeddings_index\n",
    "end_emb = time.time()\n",
    "print('       Duration:',round((end_emb-start_emb)/60,2),'min') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save = True\n",
    "MAX_SEQUENCE_LENGTH = 250 \n",
    "MAX_NB_WORDS = None\n",
    "batch_size = 10 \n",
    "epochs = 30\n",
    "filename = 'fasttext/crawl-2M_300d.vec'\n",
    "filters = '#$%&()*+-/:;<=>@[\\]^_`{|}~' #excluded: !¡ ¿? . ,\n",
    "lower = True\n",
    "    \n",
    "path = '0-publication/results_embeddings/'\n",
    "\n",
    "try_transcriptions = ['manual_Punc','manual_noPunc','manual_pauses','ASR_wav2vec2_noPunc','ASR_whisper_Punc','ASR_whisper_noPunc','ASR_whisper_pauses']\n",
    "\n",
    "df_results = pd.DataFrame()\n",
    "start = time.time() \n",
    "cleaning_training = False\n",
    "for identifier, selected_text in enumerate(try_transcriptions):\n",
    "    print('============== ', selected_text,'- clean:',cleaning_training,' ==============')\n",
    "    path_save = path + selected_text + '_' + 'cleaning'+str(cleaning_training) + '_TESTING_FINAL/'\n",
    "    makeDirIfNotExists(path_save)\n",
    "        \n",
    "    # (1) Load transcriptions\n",
    "    df_all = pd.read_excel('0-publication/transcripts_ADReSS-IS2020_selected_v3.xlsx')\n",
    "    df_all.rename(columns = {selected_text:'text', 'label':'group','user':'file'}, inplace = True)\n",
    "\n",
    "    if cleaning_training:\n",
    "        subjects_drop = ['S083', 'S093', 'S095', 'S101', 'S103', 'S104', 'S139', 'S151'] # En training, mmse>23 & label=1\n",
    "        df = df_all[~df_all['file'].isin(subjects_drop)].copy()\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "    else:\n",
    "        df = df_all.copy()\n",
    "    \n",
    "    df_train = df.loc[df.partition == 'train'].copy()\n",
    "    df_train.drop(columns='partition',inplace=True)\n",
    "    df_train = df_train.sample(frac=1, random_state=13) # shuffle\n",
    "    df_train.reset_index(drop=True, inplace=True)\n",
    "    X_train_df, Y_train_df = df_train['text'], pd.get_dummies(df_train['group']).values\n",
    "\n",
    "    df_test = df.loc[df.partition == 'test'].copy()\n",
    "    df_test.drop(columns='partition',inplace=True)\n",
    "    df_test = df_test.sample(frac=1, random_state=13) # shuffle\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "    X_test_df, Y_test_df = df_test['text'], pd.get_dummies(df_test['group']).values    \n",
    "    \n",
    "    X_dev_df, Y_dev_df = [X_train_df[0]], np.array([Y_train_df[0]]) \n",
    "    X_train, Y_train, _, _, X_test, Y_test, tokenizer = prepare_data(MAX_NB_WORDS,MAX_SEQUENCE_LENGTH,\n",
    "                                                                    filters, lower,\n",
    "                                                                    X_train_df,Y_train_df, X_dev_df, Y_dev_df, X_test_df,Y_test_df)\n",
    "\n",
    "    num_tokens, misses, misses_words, embedding_dim, embedding_matrix = load_embedding(filename, all_embeddings[filename], tokenizer)\n",
    "    \n",
    "    # (3) Testing the model\n",
    "    df_iteraciones = pd.DataFrame()\n",
    "    df_results_it = pd.DataFrame()\n",
    "    for zz in range(25):\n",
    "        model = load_model(embedding_dim, num_tokens, embedding_matrix)\n",
    "\n",
    "        if save:\n",
    "            with open(path_save+str(zz)+'_modelsummary.txt', 'w') as f:\n",
    "                model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "        history = model.fit(X_train, Y_train, \n",
    "                        epochs=epochs, batch_size=batch_size,\n",
    "                        validation_data=(X_test,Y_test),\n",
    "                        verbose = 0) #shuffle=True,callbacks=callbacks,\n",
    "        \n",
    "        # Save history of the model\n",
    "        num_epochs = range(1, len(history.history['loss']) + 1)\n",
    "        train_loss = history.history['loss']\n",
    "        train_acc = history.history['accuracy']\n",
    "        val_loss = history.history['val_loss']\n",
    "        val_acc = history.history['val_accuracy']\n",
    "        data = np.array([num_epochs,train_loss,train_acc,val_loss,val_acc])\n",
    "        df_history = pd.DataFrame(data.T, columns = ['epochs','train_loss','train_acc','val_loss','val_acc'])\n",
    "        print_evaluations(path_save, zz, history, save = True)\n",
    "        \n",
    "        y_true = []; y_pred = []\n",
    "        for a in Y_test:\n",
    "            y_true_i = np.argmax(a)\n",
    "            y_true.append(y_true_i)\n",
    "        predictions = model.predict(X_test)\n",
    "        for a in predictions:\n",
    "            y_pred_i = np.argmax(a)\n",
    "            y_pred.append(y_pred_i)\n",
    "        df_iteraciones[zz] = y_pred\n",
    "        \n",
    "        if save:\n",
    "            df_history.to_csv(path_save+str(zz)+'_history.csv',index=False)\n",
    "            model.save(path_save+str(zz)+'_model.h5')\n",
    "            save_test_results(path_save,zz,y_true,y_pred,df_test)\n",
    "            \n",
    "        test_acc, test_kappa, test_f1score, test_cm, test_sensitivity, test_specificity, test_precision = compute_metrics(y_true, y_pred)\n",
    "        errors = []\n",
    "        for i in range(len(y_true)):\n",
    "            if y_true[i] != y_pred[i]:\n",
    "                user = df_test.file.values[i]\n",
    "                errors.append(user)\n",
    "        \n",
    "        new_row =  {'text': selected_text,'it':zz,\n",
    "                   'test_acc':test_acc, 'test_kappa':test_kappa, 'test_f1score':test_f1score, 'test_cm':test_cm, \n",
    "                   'test_sensitivity':test_sensitivity, 'test_specificity':test_specificity, 'test_precision':test_precision,\n",
    "                   'errors_test':errors}\n",
    "        df_results_it = df_results_it.append(new_row, ignore_index=True)\n",
    "        df_results_it.to_csv(path_save+'it_metrics.csv')\n",
    "    \n",
    "    df_iteraciones['voting'] = 10\n",
    "    for i in range(df_iteraciones.shape[0]):\n",
    "        valores = list(df_iteraciones.loc[i,:].values)\n",
    "        maj_voting = max(set(valores), key = valores.count)\n",
    "        df_iteraciones.loc[i,'voting'] = maj_voting\n",
    "    df_iteraciones['true'] = y_true\n",
    "    df_iteraciones.to_csv(path_save+'it_predictions.csv')\n",
    "    \n",
    "    y_true = df_iteraciones['true'].values\n",
    "    y_pred = df_iteraciones['voting'].values\n",
    "    test_acc, test_kappa, test_f1score, test_cm, test_sensitivity, test_specificity, test_precision = compute_metrics(y_true, y_pred)\n",
    "    errors = []\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] != y_pred[i]:\n",
    "            user = df_test.file.values[i]\n",
    "            errors.append(user)\n",
    "            \n",
    "    new_row =  {'text': selected_text,\n",
    "               'test_acc':test_acc, 'test_kappa':test_kappa, 'test_f1score':test_f1score, 'test_cm':test_cm, \n",
    "               'test_sensitivity':test_sensitivity, 'test_specificity':test_specificity, 'test_precision':test_precision,\n",
    "               'errors_test':errors}\n",
    "    df_results = df_results.append(new_row, ignore_index=True)\n",
    "    df_results.to_csv('0-publication/results_embeddings/mean_tests.csv')        \n",
    "    df_results.to_excel('0-publication/results_embeddings/mean_tests.xlsx',index=False)  \n",
    "    \n",
    "    end = time.time()\n",
    "    print('\\n** Duration:',round((end - start)/60,2),'min')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "RlPZ0tVL-Nsc",
    "bRVlf_QH-FoW"
   ],
   "name": "Embeddings_valencia_text.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
